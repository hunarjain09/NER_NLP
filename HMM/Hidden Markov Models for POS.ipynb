{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.core.display import HTML\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict, namedtuple, OrderedDict\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution\n",
    "import os\n",
    "from io import BytesIO\n",
    "from itertools import chain\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Sentence = namedtuple(\"Sentence\", \"words\")\n",
    "\n",
    "def getMeTestSentences(data):\n",
    "    sentences = []\n",
    "    for key in data:\n",
    "        sentence = []\n",
    "        for val in zip(data[key].words):\n",
    "            sentence.append(val)\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "def read_test_data(filename):\n",
    "    \"\"\"Read tagged sentence data\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        sentence_lines = [l.split(\"\\n\") for l in f.read().split(\"\\n\\n\")]\n",
    "        index = 1\n",
    "        a = OrderedDict()\n",
    "        for s in sentence_lines:\n",
    "            temp = []\n",
    "            for l in s:\n",
    "                temp.append(l.strip().split(\"\\t\")[1:])   \n",
    "            temp2 = []\n",
    "            for val in temp:\n",
    "                if len(val) == 1:\n",
    "                    temp2.append(val[0])\n",
    "                          \n",
    "            a[index] = Test_Sentence(tuple(temp2))\n",
    "            index += 1\n",
    "        return a\n",
    "        \n",
    "class TestDataset(namedtuple(\"_TDataset\", \"sentences keys vocab X N\")):\n",
    "    def __new__(cls, tagfile, datafile, train_test_split=0.8, seed=112890):\n",
    "        sentences = read_test_data(datafile)\n",
    "        keys = tuple(sentences.keys())\n",
    "        wordset = frozenset(chain(*[s.words for s in sentences.values()]))\n",
    "        word_sequences = tuple([sentences[k].words for k in keys])\n",
    "        N = sum(1 for _ in chain(*(s.words for s in sentences.values())))\n",
    "        \n",
    "        return super().__new__(cls, dict(sentences), keys, wordset, word_sequences,N)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.sentences.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentence = namedtuple(\"Sentence\", \"words tags\")\n",
    "\n",
    "def read_data(filename):\n",
    "    \"\"\"Read tagged sentence data\"\"\"\n",
    "    with open(\"S21-gene-train.txt\", 'r') as f:\n",
    "        sentence_lines = [l.split(\"\\n\") for l in f.read().split(\"\\n\\n\")]\n",
    "        index = 1\n",
    "        a = OrderedDict()\n",
    "        for s in sentence_lines:\n",
    "            # print(s)\n",
    "            temp = []\n",
    "            for l in s:\n",
    "                temp.append(l.strip().split(\"\\t\")[1:])\n",
    "            \n",
    "            temp2 = []\n",
    "            temp3 = []\n",
    "            for val in temp:\n",
    "                if len(val) == 2:\n",
    "                    temp2.append(val[0])\n",
    "                    temp3.append(val[1])\n",
    "                          \n",
    "            a[index] = Sentence(tuple(temp2),tuple(temp3))\n",
    "            index += 1\n",
    "        return a\n",
    "\n",
    "def read_tags(filename):\n",
    "    \"\"\"Read a list of word tag classes\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        tags = f.read().split(\"\\n\")\n",
    "    return frozenset(tags)\n",
    "\n",
    "class Subset(namedtuple(\"BaseSet\", \"sentences keys vocab X tagset Y N stream\")):\n",
    "    def __new__(cls, sentences, keys):\n",
    "        word_sequences = tuple([sentences[k].words for k in keys])\n",
    "        tag_sequences = tuple([sentences[k].tags for k in keys])\n",
    "        wordset = frozenset(chain(*word_sequences))\n",
    "        tagset = frozenset(chain(*tag_sequences))\n",
    "        N = sum(1 for _ in chain(*(sentences[k].words for k in keys)))\n",
    "        stream = tuple(zip(chain(*word_sequences), chain(*tag_sequences)))\n",
    "        return super().__new__(cls, {k: sentences[k] for k in keys}, keys, wordset, word_sequences,\n",
    "                               tagset, tag_sequences, N, stream.__iter__)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.sentences.items())\n",
    "\n",
    "class Dataset(namedtuple(\"_Dataset\", \"sentences keys vocab X tagset Y training_set testing_set N stream\")):\n",
    "    def __new__(cls, tagfile, datafile, train_test_split=0.8, seed=112890):\n",
    "        tagset = read_tags(tagfile)\n",
    "        sentences = read_data(datafile)\n",
    "        keys = tuple(sentences.keys())\n",
    "        wordset = frozenset(chain(*[s.words for s in sentences.values()]))\n",
    "        word_sequences = tuple([sentences[k].words for k in keys])\n",
    "        tag_sequences = tuple([sentences[k].tags for k in keys])\n",
    "        N = sum(1 for _ in chain(*(s.words for s in sentences.values())))\n",
    "        \n",
    "        # split data into train/test sets\n",
    "        _keys = list(keys)\n",
    "        if seed is not None: random.seed(seed)\n",
    "        random.shuffle(_keys)\n",
    "        split = int(train_test_split * len(_keys))\n",
    "        training_data = Subset(sentences, _keys[:split])\n",
    "        testing_data = Subset(sentences, _keys[split:])\n",
    "        stream = tuple(zip(chain(*word_sequences), chain(*tag_sequences)))\n",
    "        return super().__new__(cls, dict(sentences), keys, wordset, word_sequences, tagset,\n",
    "                               tag_sequences, training_data, testing_data, N, stream.__iter__)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.sentences.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13796 sentences in the corpus.\n",
      "There are 11036 sentences in the training set.\n",
      "There are 2760 sentences in the testing set.\n"
     ]
    }
   ],
   "source": [
    "data = Dataset(\"tags-universal.txt\", \"brown-universal.txt\", train_test_split=0.8)\n",
    "\n",
    "print(\"There are {} sentences in the corpus.\".format(len(data)))\n",
    "print(\"There are {} sentences in the training set.\".format(len(data.training_set)))\n",
    "print(\"There are {} sentences in the testing set.\".format(len(data.testing_set)))\n",
    "\n",
    "assert len(data) == len(data.training_set) + len(data.testing_set), \\\n",
    "       \"The number of sentences in the training set + testing set should sum to the number of sentences in the corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 10\n",
      "words:\n",
      "\t('The', 'variable', 'HMG', 'dosage', 'regimen', 'was', 'found', 'to', 'offer', 'no', 'advantages', 'when', 'compared', 'with', 'our', 'standard', 'daily', 'dosage', 'regimen', '.')\n",
      "tags:\n",
      "\t('O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O')\n"
     ]
    }
   ],
   "source": [
    "key = 10\n",
    "print(\"Sentence: {}\".format(key))\n",
    "print(\"words:\\n\\t{!s}\".format(data.sentences[key].words))\n",
    "print(\"tags:\\n\\t{!s}\".format(data.sentences[key].tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 386201 samples of 31328 unique words in the corpus.\n",
      "There are 309830 samples of 27563 unique words in the training set.\n",
      "There are 76371 samples of 11825 unique words in the testing set.\n",
      "There are 3765 words in the test set that are missing in the training set.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are a total of {} samples of {} unique words in the corpus.\"\n",
    "      .format(data.N, len(data.vocab)))\n",
    "print(\"There are {} samples of {} unique words in the training set.\"\n",
    "      .format(data.training_set.N, len(data.training_set.vocab)))\n",
    "print(\"There are {} samples of {} unique words in the testing set.\"\n",
    "      .format(data.testing_set.N, len(data.testing_set.vocab)))\n",
    "print(\"There are {} words in the test set that are missing in the training set.\"\n",
    "      .format(len(data.testing_set.vocab - data.training_set.vocab)))\n",
    "\n",
    "assert data.N == data.training_set.N + data.testing_set.N, \\\n",
    "       \"The number of training + test samples should sum to the total number of samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: ('Comparison', 'with', 'alkaline', 'phosphatases', 'and', '5', '-', 'nucleotidase', '.')\n",
      "\n",
      "Labels 1: ('O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O')\n",
      "\n",
      "Sentence 2: ('Pharmacologic', 'aspects', 'of', 'neonatal', 'hyperbilirubinemia', '.')\n",
      "\n",
      "Labels 2: ('O', 'O', 'O', 'O', 'O', 'O')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accessing words with Dataset.X and tags with Dataset.Y \n",
    "for i in range(2):    \n",
    "    print(\"Sentence {}:\".format(i + 1), data.X[i])\n",
    "    print()\n",
    "    print(\"Labels {}:\".format(i + 1), data.Y[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stream (word, tag) pairs:\n",
      "\n",
      "\t ('Comparison', 'O')\n",
      "\t ('with', 'O')\n",
      "\t ('alkaline', 'B')\n",
      "\t ('phosphatases', 'I')\n",
      "\t ('and', 'O')\n"
     ]
    }
   ],
   "source": [
    "# use Dataset.stream() (word, tag) samples for the entire corpus\n",
    "print(\"\\nStream (word, tag) pairs:\\n\")\n",
    "for i, pair in enumerate(data.stream()):\n",
    "    print(\"\\t\", pair)\n",
    "    if i > 3: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknown(sequence):\n",
    "    \n",
    "    return [w if w in data.training_set.vocab else 'nan' for w in sequence]\n",
    "\n",
    "def simplify_decoding(X, model):\n",
    "    \n",
    "    _, state_path = model.viterbi(replace_unknown(X))\n",
    "    return [state[1].name for state in state_path[1:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build an HMM tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_counts(sequences):\n",
    "\n",
    "    return Counter(sequences)\n",
    "\n",
    "tags = [tag for i, (word, tag) in enumerate(data.training_set.stream())]\n",
    "tag_unigrams = unigram_counts(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_counts(sequences):\n",
    "\n",
    "    d = Counter(sequences)\n",
    "    return d\n",
    "\n",
    "tags = [tag for i, (word, tag) in enumerate(data.stream())]\n",
    "o = [(tags[i],tags[i+1]) for i in range(0,len(tags)-2,2)]\n",
    "tag_bigrams = bigram_counts(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Starting Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def starting_counts(sequences):\n",
    "    \n",
    "    d = Counter(sequences)\n",
    "    return d\n",
    "\n",
    "tags = [tag for i, (word, tag) in enumerate(data.stream())]\n",
    "starts_tag = [i[0] for i in data.Y]\n",
    "tag_starts = starting_counts(starts_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Ending Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ending_counts(sequences):\n",
    "    \n",
    "    d = Counter(sequences)\n",
    "    return d\n",
    "\n",
    "end_tag = [i[len(i)-1] for i in data.Y]\n",
    "tag_ends = ending_counts(end_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_counts(tags, words):\n",
    "    d = defaultdict(lambda: defaultdict(int))\n",
    "    for tag, word in zip(tags, words):\n",
    "        d[tag][word] += 1\n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic HMM Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = HiddenMarkovModel(name=\"base-hmm-tagger\")\n",
    "\n",
    "tags = [tag for i, (word, tag) in enumerate(data.stream())]\n",
    "words = [word for i, (word, tag) in enumerate(data.stream())]\n",
    "\n",
    "tags_count=unigram_counts(tags)\n",
    "tag_words_count=pair_counts(tags,words)\n",
    "\n",
    "starting_tag_list=[i[0] for i in data.Y]\n",
    "ending_tag_list=[i[-1] for i in data.Y]\n",
    "\n",
    "starting_tag_count=starting_counts(starting_tag_list)\n",
    "ending_tag_count=ending_counts(ending_tag_list)      \n",
    "\n",
    "\n",
    "\n",
    "to_pass_states = []\n",
    "for tag, words_dict in tag_words_count.items():\n",
    "    total = float(sum(words_dict.values()))\n",
    "    distribution = {word: count/total for word, count in words_dict.items()}\n",
    "    tag_emissions = DiscreteDistribution(distribution)\n",
    "    tag_state = State(tag_emissions, name=tag)\n",
    "    to_pass_states.append(tag_state)\n",
    "\n",
    "\n",
    "basic_model.add_states()    \n",
    "    \n",
    "\n",
    "start_prob={}\n",
    "\n",
    "for tag in tags:\n",
    "    start_prob[tag]=starting_tag_count[tag]/tags_count[tag]\n",
    "\n",
    "for tag_state in to_pass_states :\n",
    "    basic_model.add_transition(basic_model.start,tag_state,start_prob[tag_state.name])    \n",
    "\n",
    "end_prob={}\n",
    "\n",
    "for tag in tags:\n",
    "    end_prob[tag]=ending_tag_count[tag]/tags_count[tag]\n",
    "for tag_state in to_pass_states :\n",
    "    basic_model.add_transition(tag_state,basic_model.end,end_prob[tag_state.name])\n",
    "    \n",
    "\n",
    "\n",
    "transition_prob_pair={}\n",
    "\n",
    "for key in tag_bigrams.keys():\n",
    "    transition_prob_pair[key]=tag_bigrams.get(key)/tags_count[key[0]]\n",
    "for tag_state in to_pass_states :\n",
    "    for next_tag_state in to_pass_states :\n",
    "        if (tag_state.name,next_tag_state.name) in transition_prob_pair:\n",
    "            basic_model.add_transition(tag_state,next_tag_state,transition_prob_pair[(tag_state.name,next_tag_state.name)])\n",
    "        else:\n",
    "            basic_model.add_transition(tag_state,next_tag_state,0)\n",
    "\n",
    "\n",
    "basic_model.bake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, Y, model):\n",
    "    \n",
    "    correct = total_predictions = 0\n",
    "    for observations, actual_tags in zip(X, Y):\n",
    "        \n",
    "        # The model.viterbi call in simplify_decoding will return None if the HMM\n",
    "        # raises an error (for example, if a test sentence contains a word that\n",
    "        # is out of vocabulary for the training set).\n",
    "        try:\n",
    "            most_likely_tags = simplify_decoding(observations, model)\n",
    "            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))\n",
    "        except:\n",
    "            pass\n",
    "        total_predictions += len(observations)\n",
    "    return correct / total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy basic hmm model: 96.41%\n",
      "testing accuracy basic hmm model: 93.99%\n"
     ]
    }
   ],
   "source": [
    "hmm_training_acc = accuracy(data.training_set.X, data.training_set.Y, basic_model)\n",
    "print(\"training accuracy basic hmm model: {:.2f}%\".format(100 * hmm_training_acc))\n",
    "\n",
    "hmm_testing_acc = accuracy(data.testing_set.X, data.testing_set.Y, basic_model)\n",
    "print(\"testing accuracy basic hmm model: {:.2f}%\".format(100 * hmm_testing_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Decoding Sequences with the HMM Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data.testing_set.keys[:2]:\n",
    "    print(\"Sentence Key: {}\\n\".format(key))\n",
    "    print(\"Predicted labels:\\n-----------------\")\n",
    "    print(simplify_decoding(data.sentences[key].words, basic_model))\n",
    "    print()\n",
    "    print(\"Actual labels:\\n--------------\")\n",
    "    print(data.sentences[key].tags)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('yoursystemoutput.txt', 'w') as f:\n",
    "    for key in data.testing_set.sentences:\n",
    "        for i,val in enumerate(zip(data.testing_set.sentences[key].words,simplify_decoding(data.sentences[key].words, basic_model))):\n",
    "            f.write(\"\\t\".join([str(i+1),val[0],val[1]]) + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "with open('goldstandardfile.txt', 'w') as f:\n",
    "    for key in data.testing_set.sentences:\n",
    "        for i,val in enumerate(zip(data.testing_set.sentences[key].words,data.testing_set.sentences[key].tags)):\n",
    "            f.write(\"\\t\".join([str(i+1),val[0],val[1]]) + \"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TestDataset(\"tags-universal.txt\", \"S21-gene-test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('testFinal.txt', 'w') as f:\n",
    "    k = 0\n",
    "    for key in test_data.sentences:\n",
    "        for i,val in enumerate(zip(test_data.sentences[key].words,simplify_decoding(test_data.sentences[key].words, basic_model))):\n",
    "            f.write(\"\\t\".join([str(i+1),val[0],val[1]]) + \"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
