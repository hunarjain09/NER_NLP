{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.core.display import HTML\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict, namedtuple, OrderedDict\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution\n",
    "import os\n",
    "from io import BytesIO\n",
    "from itertools import chain\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Sentence = namedtuple(\"Sentence\", \"words\")\n",
    "\n",
    "def getMeTestSentences(data):\n",
    "    sentences = []\n",
    "    for key in data:\n",
    "        sentence = []\n",
    "        for val in zip(data[key].words):\n",
    "            sentence.append(val)\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "def read_test_data(filename):\n",
    "    \"\"\"Read tagged sentence data\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        sentence_lines = [l.split(\"\\n\") for l in f.read().split(\"\\n\\n\")]\n",
    "        index = 1\n",
    "        a = OrderedDict()\n",
    "        for s in sentence_lines:\n",
    "            temp = []\n",
    "            for l in s:\n",
    "                temp.append(l.strip().split(\"\\t\")[1:])   \n",
    "            temp2 = []\n",
    "            for val in temp:\n",
    "                if len(val) == 1:\n",
    "                    temp2.append(val[0])\n",
    "                          \n",
    "            a[index] = Test_Sentence(tuple(temp2))\n",
    "            index += 1\n",
    "        return a\n",
    "        \n",
    "class TestDataset(namedtuple(\"_TDataset\", \"sentences keys vocab X N\")):\n",
    "    def __new__(cls, tagfile, datafile, train_test_split=0.8, seed=112890):\n",
    "        sentences = read_test_data(datafile)\n",
    "        keys = tuple(sentences.keys())\n",
    "        wordset = frozenset(chain(*[s.words for s in sentences.values()]))\n",
    "        word_sequences = tuple([sentences[k].words for k in keys])\n",
    "        N = sum(1 for _ in chain(*(s.words for s in sentences.values())))\n",
    "        \n",
    "        return super().__new__(cls, dict(sentences), keys, wordset, word_sequences,N)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.sentences.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentence = namedtuple(\"Sentence\", \"words tags\")\n",
    "\n",
    "def read_data(filename):\n",
    "    \"\"\"Read tagged sentence data\"\"\"\n",
    "    with open(\"S21-gene-train.txt\", 'r') as f:\n",
    "        sentence_lines = [l.split(\"\\n\") for l in f.read().split(\"\\n\\n\")]\n",
    "        index = 1\n",
    "        a = OrderedDict()\n",
    "        for s in sentence_lines:\n",
    "            # print(s)\n",
    "            temp = []\n",
    "            for l in s:\n",
    "                temp.append(l.strip().split(\"\\t\")[1:])\n",
    "            \n",
    "            temp2 = []\n",
    "            temp3 = []\n",
    "            for val in temp:\n",
    "                if len(val) == 2:\n",
    "                    temp2.append(val[0])\n",
    "                    temp3.append(val[1])\n",
    "                          \n",
    "            a[index] = Sentence(tuple(temp2),tuple(temp3))\n",
    "            index += 1\n",
    "        return a\n",
    "\n",
    "def read_tags(filename):\n",
    "    \"\"\"Read a list of word tag classes\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        tags = f.read().split(\"\\n\")\n",
    "    return frozenset(tags)\n",
    "\n",
    "class Subset(namedtuple(\"BaseSet\", \"sentences keys vocab X tagset Y N stream\")):\n",
    "    def __new__(cls, sentences, keys):\n",
    "        word_sequences = tuple([sentences[k].words for k in keys])\n",
    "        tag_sequences = tuple([sentences[k].tags for k in keys])\n",
    "        wordset = frozenset(chain(*word_sequences))\n",
    "        tagset = frozenset(chain(*tag_sequences))\n",
    "        N = sum(1 for _ in chain(*(sentences[k].words for k in keys)))\n",
    "        stream = tuple(zip(chain(*word_sequences), chain(*tag_sequences)))\n",
    "        return super().__new__(cls, {k: sentences[k] for k in keys}, keys, wordset, word_sequences,\n",
    "                               tagset, tag_sequences, N, stream.__iter__)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.sentences.items())\n",
    "\n",
    "class Dataset(namedtuple(\"_Dataset\", \"sentences keys vocab X tagset Y training_set testing_set N stream\")):\n",
    "    def __new__(cls, tagfile, datafile, train_test_split=0.8, seed=112890):\n",
    "        tagset = read_tags(tagfile)\n",
    "        sentences = read_data(datafile)\n",
    "        keys = tuple(sentences.keys())\n",
    "        wordset = frozenset(chain(*[s.words for s in sentences.values()]))\n",
    "        word_sequences = tuple([sentences[k].words for k in keys])\n",
    "        tag_sequences = tuple([sentences[k].tags for k in keys])\n",
    "        N = sum(1 for _ in chain(*(s.words for s in sentences.values())))\n",
    "        \n",
    "        # split data into train/test sets\n",
    "        _keys = list(keys)\n",
    "        if seed is not None: random.seed(seed)\n",
    "        random.shuffle(_keys)\n",
    "        split = int(train_test_split * len(_keys))\n",
    "        training_data = Subset(sentences, _keys[:split])\n",
    "        testing_data = Subset(sentences, _keys[split:])\n",
    "        stream = tuple(zip(chain(*word_sequences), chain(*tag_sequences)))\n",
    "        return super().__new__(cls, dict(sentences), keys, wordset, word_sequences, tagset,\n",
    "                               tag_sequences, training_data, testing_data, N, stream.__iter__)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.sentences.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13796 sentences in the corpus.\n",
      "There are 11036 sentences in the training set.\n",
      "There are 2760 sentences in the testing set.\n"
     ]
    }
   ],
   "source": [
    "data = Dataset(\"tags-universal.txt\", \"brown-universal.txt\", train_test_split=0.8)\n",
    "\n",
    "print(\"There are {} sentences in the corpus.\".format(len(data)))\n",
    "print(\"There are {} sentences in the training set.\".format(len(data.training_set)))\n",
    "print(\"There are {} sentences in the testing set.\".format(len(data.testing_set)))\n",
    "\n",
    "assert len(data) == len(data.training_set) + len(data.testing_set), \\\n",
    "       \"The number of sentences in the training set + testing set should sum to the number of sentences in the corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 10\n",
      "words:\n",
      "\t('The', 'variable', 'HMG', 'dosage', 'regimen', 'was', 'found', 'to', 'offer', 'no', 'advantages', 'when', 'compared', 'with', 'our', 'standard', 'daily', 'dosage', 'regimen', '.')\n",
      "tags:\n",
      "\t('O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O')\n"
     ]
    }
   ],
   "source": [
    "key = 10\n",
    "print(\"Sentence: {}\".format(key))\n",
    "print(\"words:\\n\\t{!s}\".format(data.sentences[key].words))\n",
    "print(\"tags:\\n\\t{!s}\".format(data.sentences[key].tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 386201 samples of 31328 unique words in the corpus.\n",
      "There are 309830 samples of 27563 unique words in the training set.\n",
      "There are 76371 samples of 11825 unique words in the testing set.\n",
      "There are 3765 words in the test set that are missing in the training set.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are a total of {} samples of {} unique words in the corpus.\"\n",
    "      .format(data.N, len(data.vocab)))\n",
    "print(\"There are {} samples of {} unique words in the training set.\"\n",
    "      .format(data.training_set.N, len(data.training_set.vocab)))\n",
    "print(\"There are {} samples of {} unique words in the testing set.\"\n",
    "      .format(data.testing_set.N, len(data.testing_set.vocab)))\n",
    "print(\"There are {} words in the test set that are missing in the training set.\"\n",
    "      .format(len(data.testing_set.vocab - data.training_set.vocab)))\n",
    "\n",
    "assert data.N == data.training_set.N + data.testing_set.N, \\\n",
    "       \"The number of training + test samples should sum to the total number of samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: ('Comparison', 'with', 'alkaline', 'phosphatases', 'and', '5', '-', 'nucleotidase', '.')\n",
      "\n",
      "Labels 1: ('O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O')\n",
      "\n",
      "Sentence 2: ('Pharmacologic', 'aspects', 'of', 'neonatal', 'hyperbilirubinemia', '.')\n",
      "\n",
      "Labels 2: ('O', 'O', 'O', 'O', 'O', 'O')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accessing words with Dataset.X and tags with Dataset.Y \n",
    "for i in range(2):    \n",
    "    print(\"Sentence {}:\".format(i + 1), data.X[i])\n",
    "    print()\n",
    "    print(\"Labels {}:\".format(i + 1), data.Y[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stream (word, tag) pairs:\n",
      "\n",
      "\t ('Comparison', 'O')\n",
      "\t ('with', 'O')\n",
      "\t ('alkaline', 'B')\n",
      "\t ('phosphatases', 'I')\n",
      "\t ('and', 'O')\n"
     ]
    }
   ],
   "source": [
    "# use Dataset.stream() (word, tag) samples for the entire corpus\n",
    "print(\"\\nStream (word, tag) pairs:\\n\")\n",
    "for i, pair in enumerate(data.stream()):\n",
    "    print(\"\\t\", pair)\n",
    "    if i > 3: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = (tag for i, (word, tag) in enumerate(data.training_set.stream()))\n",
    "words = (word for i, (word, tag) in enumerate(data.training_set.stream()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknown(sequence):\n",
    "    \n",
    "    return [w if w in data.training_set.vocab else 'nan' for w in sequence]\n",
    "\n",
    "def simplify_decoding(X, model):\n",
    "    _, state_path = model.viterbi(replace_unknown(X))\n",
    "    if not state_path:\n",
    "        print('Hello',X,replace_unknown(X))\n",
    "    return [state[1].name for state in state_path[1:-1]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [tag for i, (word, tag) in enumerate(data.training_set.stream())]\n",
    "dict_tag = {}\n",
    "\n",
    "def unigram_counts(sequences):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the input sequence list that\n",
    "    counts the number of occurrences of the value in the sequences list. The sequences\n",
    "    collection should be a 2-dimensional array.\n",
    "    \n",
    "    For example, if the tag NOUN appears 275558 times over all the input sequences,\n",
    "    then you should return a dictionary such that your_unigram_counts[NOUN] == 275558.\n",
    "    \"\"\"\n",
    "    for tag in sequences:\n",
    "        if tag in dict_tag.keys():\n",
    "            dict_tag[tag] += 1\n",
    "        else: dict_tag[tag] = 1\n",
    "    return dict_tag\n",
    "\n",
    "\n",
    "tag_unigrams = unigram_counts(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [tag for i, (word, tag) in enumerate(data.training_set.stream())]\n",
    "sq = list(zip(tags[:-1],tags[1:]))\n",
    "dict_sq = {}\n",
    "\n",
    "def bigram_counts(sequences):\n",
    "    \"\"\"Return a dictionary keyed to each unique PAIR of values in the input sequences\n",
    "    list that counts the number of occurrences of pair in the sequences list. The input\n",
    "    should be a 2-dimensional array.\n",
    "    \n",
    "    For example, if the pair of tags (NOUN, VERB) appear 61582 times, then you should\n",
    "    return a dictionary such that your_bigram_counts[(NOUN, VERB)] == 61582\n",
    "    \"\"\"\n",
    "\n",
    "    for tag_pair in sequences:\n",
    "        if tag_pair in dict_sq.keys():\n",
    "            dict_sq[tag_pair] += 1\n",
    "        else: dict_sq[tag_pair] = 1\n",
    "    return dict_sq\n",
    "\n",
    "tag_bigrams = bigram_counts(sq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_starting = defaultdict(list)\n",
    "def starting_counts(sequences):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the input sequences list\n",
    "    that counts the number of occurrences where that value is at the beginning of\n",
    "    a sequence.\n",
    "    \n",
    "    For example, if 8093 sequences start with NOUN, then you should return a\n",
    "    dictionary such that your_starting_counts[NOUN] == 8093\n",
    "    \"\"\"\n",
    "    for tag in data.training_set.tagset:\n",
    "        dict_starting[tag] = len([seq[0] for seq in sequences if seq[0]==tag])\n",
    "    return dict_starting\n",
    "\n",
    "tag_starts = starting_counts(data.training_set.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ending = defaultdict(list)\n",
    "\n",
    "def ending_counts(sequences):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the input sequences list\n",
    "    that counts the number of occurrences where that value is at the end of\n",
    "    a sequence.\n",
    "    \n",
    "    For example, if 18 sequences end with DET, then you should return a\n",
    "    dictionary such that your_starting_counts[DET] == 18\n",
    "    \"\"\"\n",
    "    for tag in data.training_set.tagset:\n",
    "        dict_ending[tag] = len([seq[-1] for seq in sequences if seq[-1]==tag])\n",
    "    return dict_ending\n",
    "\n",
    "tag_ends = ending_counts(data.training_set.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = HiddenMarkovModel(name=\"base-hmm-tagger\")\n",
    "\n",
    "count_tag_and_word = pair_counts(tags, words)\n",
    "states = {}\n",
    "for tag, word_dict in count_tag_and_word.items(): #data.training_set.tagset\n",
    "    p_words_given_tag_state = defaultdict(float)\n",
    "    # for each tag/word, calculate P(word|tag)\n",
    "    for word in word_dict.keys(): # data.training_set.vocab\n",
    "        p_words_given_tag_state[word] = count_tag_and_word[tag][word] / tag_unigrams[tag] \n",
    "    # create a new state for each tag from the dict of words that represents P(word|tag)\n",
    "    emission = DiscreteDistribution(dict(p_words_given_tag_state))\n",
    "    states[tag] = State(emission, name=tag)\n",
    "    \n",
    "basic_model.add_states(list(states.values()))\n",
    "\n",
    "# Adding start states\n",
    "for tag in data.training_set.tagset:\n",
    "    state = states[tag]\n",
    "    basic_model.add_transition(basic_model.start, state, tag_starts[tag]/len(data.training_set))\n",
    "\n",
    "# Adding end states\n",
    "for tag in data.training_set.tagset:\n",
    "    state = states[tag]\n",
    "    basic_model.add_transition(state, basic_model.end, tag_ends[tag]/tag_unigrams[tag])\n",
    "\n",
    "# Adding pairs\n",
    "for tag1 in data.training_set.tagset:\n",
    "    state1 = states[tag1]\n",
    "    for tag2 in data.training_set.tagset:\n",
    "        if (tag1, tag2) in tag_bigrams:\n",
    "            state2 = states[tag2]\n",
    "            basic_model.add_transition(state1, state2, tag_bigrams[(tag1, tag2)]/tag_unigrams[tag1])\n",
    "\n",
    "# finalize the model\n",
    "basic_model.bake()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Decoding Sequences with the HMM Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Key: 10080\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O')\n",
      "\n",
      "\n",
      "Sentence Key: 9941\n",
      "\n",
      "Predicted labels:\n",
      "-----------------\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "('B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in data.testing_set.keys[:2]:\n",
    "    print(\"Sentence Key: {}\\n\".format(key))\n",
    "    print(\"Predicted labels:\\n-----------------\")\n",
    "    print(simplify_decoding(data.sentences[key].words, basic_model))\n",
    "    print()\n",
    "    print(\"Actual labels:\\n--------------\")\n",
    "    print(data.sentences[key].tags)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknown(sequence):\n",
    "    \n",
    "    return [w if w in data.training_set.vocab else 'nan' for w in sequence]\n",
    "\n",
    "def simplify_decoding(X, model):\n",
    "    _, state_path = model.viterbi(replace_unknown(X))\n",
    "    if not state_path:\n",
    "        print('Hello',X,replace_unknown(X))\n",
    "    return [state[1].name for state in state_path[1:-1]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello ('Characterization', 'and', 'hormonal', 'regulation', 'of', 'the', 'promoter', 'of', 'the', 'rat', 'prostaglandin', 'endoperoxide', 'synthase', '2', 'gene', 'in', 'granulosa', 'cells', '.')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/50/pk94nxmj2jg9s_3zm3j3nyy40000gq/T/ipykernel_20041/198712579.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yoursystemoutput.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msimplify_decoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasic_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/50/pk94nxmj2jg9s_3zm3j3nyy40000gq/T/ipykernel_20041/3231742540.py\u001b[0m in \u001b[0;36msimplify_decoding\u001b[0;34m(X, model)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Hello'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "with open('yoursystemoutput.txt', 'w') as f:\n",
    "    for key in data.testing_set.sentences:\n",
    "        for i,val in enumerate(zip(data.testing_set.sentences[key].words,simplify_decoding(data.sentences[key].words, basic_model))):\n",
    "            f.write(\"\\t\".join([str(i+1),val[0],val[1]]) + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "with open('goldstandardfile.txt', 'w') as f:\n",
    "    for key in data.testing_set.sentences:\n",
    "        for i,val in enumerate(zip(data.testing_set.sentences[key].words,data.testing_set.sentences[key].tags)):\n",
    "            f.write(\"\\t\".join([str(i+1),val[0],val[1]]) + \"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TestDataset(\"tags-universal.txt\", \"S21-gene-test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('testFinal.txt', 'w') as f:\n",
    "    k = 0\n",
    "    for key in test_data.sentences:\n",
    "        for i,val in enumerate(zip(test_data.sentences[key].words,simplify_decoding(test_data.sentences[key].words, basic_model))):\n",
    "            f.write(\"\\t\".join([str(i+1),val[0],val[1]]) + \"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
